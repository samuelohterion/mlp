#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass amsart
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Multilayer-Perceptron
\end_layout

\begin_layout Part
Notation
\end_layout

\begin_layout Section
Layers
\end_layout

\begin_layout Subsection
\begin_inset Formula $L$
\end_inset

 ...
 number of layers:
\end_layout

\begin_layout Standard
A MLP contains of L layers.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
L & \in & 2+\mathbb{N}_{\text{0}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Neurons
\end_layout

\begin_layout Standard
Every layer of a MLP contains a certain amount of neurons.
 
\begin_inset Formula $\widetilde{N}$
\end_inset

 is a vector which elements gives the count of the neurons in every layer.
 So 
\begin_inset Formula $_{1}\widetilde{N}$
\end_inset

 is the number of input neurons or neurons in the first ( the input ) layer.
 
\begin_inset Formula $_{L}\widetilde{N}$
\end_inset

 is the number of the neurons in the output layer, the last one, of course.
\end_layout

\begin_layout Subsection
\begin_inset Formula $\widetilde{N}$
\end_inset

 ...
 number of neurons in layer 
\begin_inset Formula $l$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\widetilde{N} & = & \begin{pmatrix}_{1}\widetilde{N}\\
\vdots\\
_{L}\widetilde{N}
\end{pmatrix}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset Formula $N$
\end_inset

 ...
 number of neurons in layer 
\begin_inset Formula $l$
\end_inset

 plus on-neuron:
\end_layout

\begin_layout Standard
These numbers are model numbers and only by design.
 Every layer except of the output layer gets an additional on-neuron.
 So the real numbers of neurons for implementation are then a vector called
 
\begin_inset Formula $N$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
_{l}N & = & \begin{cases}
_{l}\widetilde{N}+1 & 1\le l<L\\
_{l}\widetilde{N} & L
\end{cases}\\
N & = & \widetilde{N}+\begin{pmatrix}1\\
\vdots\\
1\\
0
\end{pmatrix}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset Formula $_{l}o$
\end_inset

 ...
 output of neurons in layer 
\begin_inset Formula $l$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
_{l}o & = & \begin{pmatrix}_{l}o_{1}\\
\vdots\\
_{l}o_{\left(_{l}N\right)}
\end{pmatrix}\\
o & = & \left(\begin{pmatrix}_{1}o_{1}\\
\vdots\\
_{1}o_{\left(_{1}N\right)}
\end{pmatrix},\cdots,\begin{pmatrix}_{L}o_{1}\\
\vdots\\
_{L}o_{\left(_{L}N\right)}
\end{pmatrix}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Weights
\end_layout

\begin_layout Subsection
\begin_inset Formula $_{l}w_{t}^{f}$
\end_inset

 ...
 weight from neuron 
\begin_inset Formula $f$
\end_inset

 in layer 
\begin_inset Formula $l$
\end_inset

 to neuron 
\begin_inset Formula $t$
\end_inset

 in layer 
\begin_inset Formula $l+1$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
w & = & \begin{pmatrix}\begin{pmatrix}_{1}w_{1}^{1} & \cdots & _{1}w_{1}^{\left(_{1}N\right)}\\
\vdots & \diagdown & \vdots\\
_{1}w_{\left(_{2}N\right)}^{1} & \cdots & _{1}w_{\left(_{2}N\right)}^{\left(_{1}N\right)}
\end{pmatrix} & \cdots & \begin{pmatrix}_{\left(L-1\right)}w_{1}^{1} & \cdots & _{\left(L-1\right)}w_{1}^{\left(_{\left(L-1\right)}N\right)}\\
\vdots & \diagdown & \vdots\\
_{\left(L-1\right)}w_{\left(_{L}N\right)}^{1} & \cdots & _{\left(L-1\right)}w_{\left(_{L}N\right)}^{\left(_{\left(L-1\right)}N\right)}
\end{pmatrix}\end{pmatrix}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Net sum
\end_layout

\begin_layout Subsection
\begin_inset Formula $_{l+1}s_{t}$
\end_inset

 ...
 net sum of neuron 
\begin_inset Formula $t$
\end_inset

 in layer 
\begin_inset Formula $l+1$
\end_inset

:
\end_layout

\begin_layout Standard
For every neuron except the neurons in the input layer a net sum has to
 be calculated by the weights and the outputs of the neurons in the layer
 below.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
_{l+1}s_{t} & = & _{l}w_{t}^{f}\,_{l}o_{f}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Note!
\end_layout

\begin_layout Standard
If an index appears twice one right hand of the equation but not on the
 left hand then einstein sum is applied.
 So here it's not applied for 
\begin_inset Formula $l$
\end_inset

 and 
\begin_inset Formula $t$
\end_inset

 but for 
\begin_inset Formula $f$
\end_inset

.
 In this case on can write it:
\begin_inset Formula 
\begin{eqnarray*}
_{l+1}s_{t} & = & _{l}w_{t}\,_{l}o
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
and finally for all sums in the layer 
\begin_inset Formula $l+1$
\end_inset

 as a vector:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
_{l+1}s & = & _{l}w\,_{l}o
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset Formula $a\left(s\right)$
\end_inset

 ...
 the activation function as a modified sigmoid function 
\begin_inset Formula $sig\left(s\right)$
\end_inset

:
\end_layout

\begin_layout Subsubsection
\begin_inset Formula $min$
\end_inset

, 
\begin_inset Formula $max$
\end_inset

, 
\begin_inset Formula $rng$
\end_inset

 ...
 the range for the input output signals.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\left\{ min,max,rng\right\}  & \in & \mathbb{R}\\
min & < & max\\
rng & = & max-min
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
In most cases one will use 
\begin_inset Formula $max=1$
\end_inset

 and 
\begin_inset Formula $min=-1$
\end_inset

 or 
\begin_inset Formula $min=0$
\end_inset

.
\end_layout

\begin_layout Subsubsection
\begin_inset Formula $sig\left(s\right)$
\end_inset

 ...
 the sigmoid function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
sig & = & sig\left(s\right)\\
sig & = & \frac{1}{1+e^{-s}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection
\begin_inset Formula $sig'\left(s\right)$
\end_inset

 ...
 the derivation of the sigmoid function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
sig' & = & sig'\left(s\right)\\
 & = & \frac{e^{-s}}{\left(1+e^{-s}\right)^{2}}\\
 & = & \frac{1+e^{-s}-1}{\left(1+e^{-s}\right)^{2}}\\
 & = & \frac{1+e^{-s}}{\left(1+e^{-s}\right)^{2}}-\frac{1}{\left(1+e^{-s}\right)^{2}}\\
 & = & sig-sig^{2}\\
 & = & \left(1-sig\right)sig
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection
\begin_inset Formula $a\left(s\right)$
\end_inset

 ...
 activation function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
a & = & a\left(s\right)\\
 & = & min+\frac{rng}{1+e^{-s}}\\
 & = & min+rng\,sig\\
sig & = & \frac{a-min}{rng}\\
\mathbf{} & = & \frac{a-min}{max-min}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsubsection
\begin_inset Formula $a'\left(s\right)$
\end_inset

 ...
 the derivation of the activation function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
a' & = & a'\left(s\right)\\
 & = & rng\,sig'\\
 & = & rng\,\left(1-sig\right)sig\\
 & = & rng\,\left(1-\frac{a-min}{rng}\right)\frac{a-min}{rng}\\
 & = & \left(a-min\right)\,\left(1-\frac{a-min}{rng}\right)\\
 & = & \left(a-min\right)\,\left(\frac{rng-\left(a-min\right)}{rng}\right)\\
 & = & \frac{\left(a-min\right)}{rng}\,\left(rng-\left(a-min\right)\right)\\
 & = & \frac{\left(a-min\right)\left(max-a\right)}{max-min}\\
 & = & \frac{\left(max-a\right)\left(a-min\right)}{max-min}\\
 & = & \frac{\left(a-max\right)\left(min-a\right)}{max-min}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Part
Delta-Rule
\end_layout

\begin_layout Section
Teacher Vector
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
t & = & \begin{pmatrix}t_{1}\\
\vdots\\
t_{\left(_{L}N\right)}
\end{pmatrix}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
_{L}E & = & \frac{1}{2}\left(t-_{L}o\right)^{\dagger}\left(t-_{L}o\right)\\
 & = & \frac{1}{2}\left(t^{\dagger}t+_{L}o^{\dagger}{}_{L}o-2t^{\dagger}\,_{L}o\right)\\
 & = & \frac{1}{2}\left(\left|t\right|^{2}+\left|_{L}o\right|^{2}\right)-t^{\dagger}\,_{L}o
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\frac{\partial_{L}E}{\partial_{L}o} & = & \frac{1}{2}\left(\left(-_{L}1\right)^{\dagger}\left(t-_{L}o\right)+\left(t-_{L}o\right)^{\dagger}\left(-_{L}1\right)\right)\\
 & = & -_{L}1\left(t-_{L}o\right)\\
 & = & _{L}o-t
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
_{l+1}s & = & _{l}w\,_{l}o\\
_{l+1}s_{t} & = & _{l}w_{t}^{f}\,_{l}o_{f}\\
\\
\frac{\partial_{l+1}s_{t}}{\partial_{l}w_{t}^{f}} & = & _{l}o_{f}\\
\frac{\partial_{l+1}s_{n}}{\partial_{l}o_{f}} & = & _{l}w_{f}^{t}
\end{eqnarray*}

\end_inset


\end_layout

\end_body
\end_document
